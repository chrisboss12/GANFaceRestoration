{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff09778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import rand, randn, randint\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Concatenate, Dropout, BatchNormalization, LeakyReLU, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb0efe7",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9857f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/chris/2023-mcm-master/src/data/masked_images_split/train'\n",
    "test_dir = 'C:/Users/chris/2023-mcm-master/src/data/masked_images_split/test'\n",
    "masked_file_pattern = data_dir + '/**/*_masked.png'\n",
    "masked_file_pattern_test = test_dir + '/**/*_masked.png'\n",
    "\n",
    "orig_data_dir = 'C:/Users/chris/2023-mcm-master/src/data/dataset_split/train'\n",
    "orig_test_dir = 'C:/Users/chris/2023-mcm-master/src/data/dataset_split/test'\n",
    "orig_file_pattern = orig_data_dir + '/**/*.png'\n",
    "orig_file_pattern_test = orig_test_dir + '/**/*.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f49fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The facade training set consist of 400 images\n",
    "BUFFER_SIZE = 400\n",
    "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
    "BATCH_SIZE = 1\n",
    "# Each image is 256x256 in size\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66acd6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(input_image_file, real_image_file):\n",
    "    input_image = tf.io.read_file(input_image_file)\n",
    "    input_image = tf.io.decode_png(input_image)\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "\n",
    "    real_image = tf.io.read_file(real_image_file)\n",
    "    real_image = tf.io.decode_png(real_image)\n",
    "    real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "\n",
    "# Generates a random list of images from dataset to use in training batches\n",
    "# Takes damaged versions of images that will be used to generate fake images\n",
    "def generate_real_samples(orig_dir, masked_dir, dataset_list, batch_size):\n",
    "    X = dataset_list\n",
    "    # Declare arrays\n",
    "    original_images = []\n",
    "    damaged_images = []\n",
    "\n",
    "    # Pick random samples\n",
    "    ix = random.sample(X, batch_size)\n",
    "\n",
    "    for i in ix:\n",
    "        # Read in original images\n",
    "        image_real = cv2.imread(os.path.join(orig_dir, i))\n",
    "        original_images.append(image_real)\n",
    "        # Read in damaged versions\n",
    "        image_damaged = cv2.imread(os.path.join(masked_dir, i))\n",
    "        damaged_images.append(image_damaged)\n",
    "\n",
    "    original_images = np.asarray(original_images)\n",
    "    original_images = (original_images - 127.5) / 127.5\n",
    "    damaged_images = np.asarray(damaged_images)\n",
    "    damaged_images = (damaged_images - 127.5) / 127.5\n",
    "\n",
    "    return original_images, damaged_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "078fed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dbf7cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the images to [-1, 1]\n",
    "def normalize(input_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    \n",
    "    return input_image\n",
    "\n",
    "#def normalize(input_image, real_image):\n",
    "#    input_image = (input_image / 127.5) - 1\n",
    "#    real_image = (real_image / 127.5) - 1\n",
    "#    \n",
    "#    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "66bcaee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "    input_image = load(image_file)\n",
    "    input_image = resize(input_image, IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image = normalize(input_image)\n",
    "\n",
    "    return input_image\n",
    "\n",
    "#def load_image_train(image_file):\n",
    "#    input_image, real_image = load(image_file)\n",
    "#    input_image, real_image = resize(input_image, real_image,\n",
    "#                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "#    input_image, real_image = normalize(input_image, real_image)\n",
    "#\n",
    "#    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1c94bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "    input_image = load(image_file)\n",
    "    input_image = resize(input_image, IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image = normalize(input_image)\n",
    "\n",
    "    return input_image\n",
    "\n",
    "#def load_image_test(image_file):\n",
    "#    input_image, real_image = load(image_file)\n",
    "#    input_image, real_image = resize(input_image, real_image,\n",
    "#                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "#    input_image, real_image = normalize(input_image, real_image)\n",
    "#\n",
    "#    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "315bca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(masked_file_pattern)\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e3c9982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.list_files(masked_file_pattern_test)\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5cc29",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ee72221",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "# Define the downsample function for the generator model\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    # Weight initialization\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                               kernel_initializer=initializer, use_bias=False))\n",
    "  \n",
    "    # Apply batch normalization if required and add leaky ReLU activation function to the model\n",
    "    if apply_batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "# Define the upsample function for the generator model\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    # Weight initialization\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "  \n",
    "    # Apply batch normalization if required and add ReLU activation function to the model\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # Apply dropout if required\n",
    "    if apply_dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "input = 128\n",
    "\n",
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "    \n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "        downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "        downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "        downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "        downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "        downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "        downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "        downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "    \n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "        upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "        upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    \n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "        \n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "        \n",
    "    x = last(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41f00e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100\n",
    "\n",
    "# Define the generator model\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Define the loss function for the generator model \n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    # Binary cross-entropy loss function for the generator model\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error loss function for the generator model \n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    # Total generator loss = gan loss + (LAMBDA * l1 loss) \n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    # Return the total generator loss, gan loss, and l1 loss\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9b20fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential_91 (Sequential)     (None, 128, 128, 64  3072        ['input_7[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " sequential_92 (Sequential)     (None, 64, 64, 128)  131584      ['sequential_91[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_93 (Sequential)     (None, 32, 32, 256)  525312      ['sequential_92[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_94 (Sequential)     (None, 16, 16, 512)  2099200     ['sequential_93[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_95 (Sequential)     (None, 8, 8, 512)    4196352     ['sequential_94[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_96 (Sequential)     (None, 4, 4, 512)    4196352     ['sequential_95[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_97 (Sequential)     (None, 2, 2, 512)    4196352     ['sequential_96[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_98 (Sequential)     (None, 1, 1, 512)    4196352     ['sequential_97[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_99 (Sequential)     (None, 2, 2, 512)    4196352     ['sequential_98[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 2, 2, 1024)   0           ['sequential_99[0][0]',          \n",
      "                                                                  'sequential_97[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_100 (Sequential)    (None, 4, 4, 512)    8390656     ['concatenate_23[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 4, 4, 1024)   0           ['sequential_100[0][0]',         \n",
      "                                                                  'sequential_96[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_101 (Sequential)    (None, 8, 8, 512)    8390656     ['concatenate_24[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 8, 8, 1024)   0           ['sequential_101[0][0]',         \n",
      "                                                                  'sequential_95[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_102 (Sequential)    (None, 16, 16, 512)  8390656     ['concatenate_25[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 16, 16, 1024  0           ['sequential_102[0][0]',         \n",
      "                                )                                 'sequential_94[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_103 (Sequential)    (None, 32, 32, 256)  4195328     ['concatenate_26[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 32, 32, 512)  0           ['sequential_103[0][0]',         \n",
      "                                                                  'sequential_93[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_104 (Sequential)    (None, 64, 64, 128)  1049088     ['concatenate_27[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 64, 64, 256)  0           ['sequential_104[0][0]',         \n",
      "                                                                  'sequential_92[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_105 (Sequential)    (None, 128, 128, 64  262400      ['concatenate_28[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 128, 128, 12  0           ['sequential_105[0][0]',         \n",
      "                                8)                                'sequential_91[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_55 (Conv2DTra  (None, 256, 256, 3)  6147       ['concatenate_29[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,425,859\n",
      "Trainable params: 54,414,979\n",
      "Non-trainable params: 10,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_model = Generator()\n",
    "tmp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3de7ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss functions\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Define the global discriminator model\n",
    "def Global_Discriminator(input_shape=(256, 256, 3)):\n",
    "    # Weight initialization\n",
    "    init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    # Define the input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Define the convolutional layers\n",
    "    conv1 = tf.keras.layers.Conv2D(32, 5, strides=2, padding='same', kernel_initializer=init)(inputs)\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    \n",
    "    conv2 = tf.keras.layers.Conv2D(64, 5, strides=2, padding='same', kernel_initializer=init)(conv1)\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    \n",
    "    conv3 = tf.keras.layers.Conv2D(128, 5, strides=2, padding='same', kernel_initializer=init)(conv2)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv3)\n",
    "    \n",
    "    conv4 = tf.keras.layers.Conv2D(256, 5, strides=2, padding='same', kernel_initializer=init)(conv3)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv4)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv2D(256, 5, strides=2, padding='same', kernel_initializer=init)(conv4)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.LeakyReLU(alpha=0.2)(conv4)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(conv4)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Define the discriminator model\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the PatchGAN discriminator model\n",
    "def PGAN_Discriminator(input_shape=(256, 256, 3)):\n",
    "    # Weight initialisation\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # Define the input layer\n",
    "    inp = tf.keras.layers.Input(shape=input_shape, name='input_image')\n",
    "    # Define the target layer\n",
    "    tar = tf.keras.layers.Input(shape=input_shape, name='target_image')\n",
    "\n",
    "    # Concatenate the input and target images\n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "    # Define the convolutional layers\n",
    "    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "  \n",
    "    # Define the batch normalisation layer\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    # Define the leaky ReLU layer\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "    # Define the last convolutional layer\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "  \n",
    "    # Define the discriminator model\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7dfa7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator loss function (for the discriminator)\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    # Define the loss function for the generated images, (fake) images are 0 (fake) and 1 (real) is real\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Define the total loss\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    # Return the total loss\n",
    "    return total_disc_loss\n",
    "\n",
    "\n",
    "# Define the discriminator accuracy function (for the discriminator)\n",
    "def discriminator_accuracy(disc_real_output, disc_generated_output):\n",
    "    # Define the accuracy function for the generated images, (fake) images are 0 (fake) and 1 (real) is real\n",
    "    real_accuracy = tf.reduce_mean(tf.cast(disc_real_output > 0.5, tf.float32))\n",
    "    generated_accuracy = tf.reduce_mean(tf.cast(disc_generated_output < 0.5, tf.float32))\n",
    "\n",
    "    # Define the total accuracy \n",
    "    return (real_accuracy + generated_accuracy) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0eebba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 128, 128, 32)      2432      \n",
      "                                                                 \n",
      " leaky_re_lu_56 (LeakyReLU)  (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " conv2d_58 (Conv2D)          (None, 64, 64, 64)        51264     \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 64, 64, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_57 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 32, 32, 128)       204928    \n",
      "                                                                 \n",
      " batch_normalization_100 (Ba  (None, 32, 32, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 16, 16, 256)       819456    \n",
      "                                                                 \n",
      " batch_normalization_101 (Ba  (None, 16, 16, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 8, 8, 256)         1638656   \n",
      "                                                                 \n",
      " batch_normalization_102 (Ba  (None, 8, 8, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_60 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               8389120   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,109,185\n",
      "Trainable params: 11,107,777\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " target_image (InputLayer)      [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 256, 256, 6)  0           ['input_image[0][0]',            \n",
      "                                                                  'target_image[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_106 (Sequential)    (None, 128, 128, 64  6144        ['concatenate_30[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " sequential_107 (Sequential)    (None, 64, 64, 128)  131584      ['sequential_106[0][0]']         \n",
      "                                                                                                  \n",
      " sequential_108 (Sequential)    (None, 32, 32, 256)  525312      ['sequential_107[0][0]']         \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 34, 34, 256)  0          ['sequential_108[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 31, 31, 512)  2097152     ['zero_padding2d[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 31, 31, 512)  2048       ['conv2d_65[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 31, 31, 512)  0           ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 33, 33, 512)  0          ['leaky_re_lu_64[0][0]']         \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 30, 30, 1)    8193        ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,770,433\n",
      "Trainable params: 2,768,641\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Global_Discriminator()\n",
    "pgan_discriminator = PGAN_Discriminator()\n",
    "\n",
    "discriminator.summary()\n",
    "pgan_discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7e8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN model\n",
    "def define_gan(g_model, dpatch_model, dglobal_model, in_shape=(256, 256, 3)):\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in dpatch_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    for layer in dglobal_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    # define the source image\n",
    "    in_src = Input(shape=in_shape)\n",
    "\n",
    "    # connect the source image to the generator input\n",
    "    gen_out = g_model(in_src)\n",
    "\n",
    "    # connect the source input and generator output to the discriminator input\n",
    "    disglobal_out = dglobal_model(gen_out)\n",
    "    dispatch_out = dpatch_model(gen_out)\n",
    "\n",
    "    # src image as input, generated image and classification output\n",
    "    model = Model(in_src, [dispatch_out, disglobal_out, gen_out])\n",
    "\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'binary_crossentropy', 'mse'], optimizer=opt, loss_weights=[1, 1, 1000])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ba8a1",
   "metadata": {},
   "source": [
    "### Saving Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4e629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved samples generated by the GAN, original images, damaged images and generated images for comparison\n",
    "def saveGeneratedSamples(original_images, recon_images, groundtruth_images, epoch):\n",
    "    n = 5\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for i in range(0, n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(3, n, i + 1)\n",
    "        plt.imshow(np.flip(((original_images + 1) / 2)[i], axis=-1))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        # Title\n",
    "        ax.title.set_text('Original Images')\n",
    "\n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(3, n, i + 1 + 5)\n",
    "        plt.imshow(np.flip(((recon_images + 1) / 2)[i], axis=-1))\n",
    "        # plt.imshow(((recon_images + 1) / 2)[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        # Title\n",
    "        ax.title.set_text('Generated Images')\n",
    "\n",
    "        # Display Ground Truth\n",
    "        ax = plt.subplot(3, n, i + 1 + 10)\n",
    "        plt.imshow(np.flip(((groundtruth_images + 1) / 2)[i], axis=-1))\n",
    "        # plt.imshow(((groundtruth_images + 1) / 2)[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        # Title\n",
    "        ax.title.set_text('Ground Truth Images')\n",
    "    filename = 'epoch_' + str(epoch) + '_images.png'\n",
    "    plt.savefig(os.path.join('Images', filename))\n",
    "    print('Image saved')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ba0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models so as not to lose progress during training\n",
    "def saveModels(d_patch, d_global, gan):\n",
    "    d_patch.save('Models/d_patch.h5', overwrite=True)\n",
    "    d_global.save('Models/d_global.h5', overwrite=True)\n",
    "    gan.save('Models/gan.h5', overwrite=True)\n",
    "\n",
    "def saveGenModel(gen_model, epoch):\n",
    "    gen_model.save('Models/g_model_epoch' + str(epoch) + '.h5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c98f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save graphs\n",
    "def plot_discriminator(dlossreal, dlossfake, dlossreal2, dlossfake2, test=False):\n",
    "    plt.plot(dlossreal)\n",
    "    plt.plot(dlossfake)\n",
    "    plt.plot(dlossreal2)\n",
    "    plt.plot(dlossfake2)\n",
    "    plt.title('Discriminator Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    if test:\n",
    "        plt.legend(['PatchGAN Discriminator test real loss', 'PatchGAN Discriminator test fake loss',\n",
    "                    'Global Discriminator test real loss', 'Global Discriminator test fake loss'],\n",
    "                   loc='lower left')\n",
    "        filename = 'Graphs/discriminator_test_loss_graph_testing.png'\n",
    "    else:\n",
    "        plt.legend(['PatchGAN Discriminator train real loss', 'PatchGAN Discriminator train fake loss',\n",
    "                    'Global Discriminator train real loss', 'Global Discriminator train fake loss'],\n",
    "                   loc='lower left')\n",
    "        filename = 'Graphs/discriminator_loss_graph_testing2.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_GAN(glossbce, glossbce2, glossmse, test=False):\n",
    "    plt.plot(glossbce)\n",
    "    plt.plot(glossbce2)\n",
    "    plt.plot(glossmse)\n",
    "    plt.title('GAN Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    if test:\n",
    "        plt.legend(['GAN test PatchGAN BCE loss', 'GAN test Global BCE loss', 'GAN test MSE loss'], loc='lower left')\n",
    "        filename = 'Graphs/GAN_test_loss_graph_testing.png'\n",
    "    else:\n",
    "        plt.legend(['GAN train PatchGAN BCE loss', 'GAN train Global BCE loss', 'GAN train MSE loss'], loc='lower left')\n",
    "        filename = 'Graphs/GAN_loss_graph_testing2.png'\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9dd0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "pgan_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9093004",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"C:/Users/chris/2023-mcm-master/src/data/training_checkpoints/\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 pgan_discriminator_optimizer=pgan_discriminator_optimizer, \n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator,\n",
    "                                 pgan_discriminator=pgan_discriminator\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2aabb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Generate images\n",
    "################################################################\n",
    "\n",
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "de6d9e64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [127]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example_input, example_target \u001b[38;5;129;01min\u001b[39;00m test_dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     generate_images(generator, example_input, example_target)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m---------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mValueError                                Traceback (most recent call last)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03mno real images yet (26/06/23)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "for example_input, example_target in test_dataset.take(1):\n",
    "    generate_images(generator, example_input, example_target)\n",
    "    \n",
    "\"\"\"\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Input In [126], in <cell line: 1>()\n",
    "----> 1 for example_input, example_target in test_dataset.take(1):\n",
    "      2     generate_images(generator, example_input, example_target)\n",
    "\n",
    "ValueError: not enough values to unpack (expected 2, got 1)\n",
    "\n",
    "--> Error is like this because we only have the masked images, \n",
    "no real images yet -26/06/23-\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea82956",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dea2732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "302a5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, step):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da903bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_ds, test_ds, steps):\n",
    "  example_input, example_target = next(iter(test_ds.take(1)))\n",
    "  start = time.time()\n",
    "\n",
    "  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
    "    if (step) % 1000 == 0:\n",
    "      display.clear_output(wait=True)\n",
    "\n",
    "      if step != 0:\n",
    "        print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "      start = time.time()\n",
    "\n",
    "      generate_images(generator, example_input, example_target)\n",
    "      print(f\"Step: {step//1000}k\")\n",
    "\n",
    "    train_step(input_image, target, step)\n",
    "\n",
    "    # Training step\n",
    "    if (step+1) % 10 == 0:\n",
    "      print('.', end='', flush=True)\n",
    "\n",
    "\n",
    "    # Save (checkpoint) the model every 5k steps\n",
    "    if (step + 1) % 5000 == 0:\n",
    "      checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy')\n",
    "#tmp_model.fit(train_x, train_y,\n",
    "#                epochs=50,\n",
    "#                batch_size=32,\n",
    "#                 shuffle=True,\n",
    "#                 validation_data=(test_x, y_test),\n",
    "#                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7754f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

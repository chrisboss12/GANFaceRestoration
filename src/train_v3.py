# -*- coding: utf-8 -*-
"""train2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_-FhVtZ546hE8bS-QYJlEBWK-YCdehb9
"""

import os
import numpy as np
import random
import cv2
import time
import datetime
import matplotlib.pyplot as plt
import tensorflow as tf
from numpy import zeros, ones
from matplotlib.axis import datetime
from skimage.metrics import peak_signal_noise_ratio, structural_similarity

tf.debugging.set_log_device_placement(True) 
tf.disable_v2_behavior()


"""## Facial image restoration task
### Dataset Loading
"""

data_dir = '/mnt/c/Users/chris/2023-mcm-master/src/data/practicum_dataset_split/train'
masked_dir = '/mnt/c//Users/chris/2023-mcm-master/src/data/masked_images_split/train'

data_dir_test = '/mnt/c/Users/chris/2023-mcm-master/src/data/practicum_dataset_split/test'
masked_dir_test = '/mnt/c/Users/chris/2023-mcm-master/src/data/masked_images_split/test'

data_dir_val = '/mnt/c/Users/chris/2023-mcm-master/src/data/practicum_dataset_split/val'
masked_dir_val = '/mnt/c/Users/chris/2023-mcm-master/src/data/masked_images_split/val'

print('Train dataset:')
image_names_train = []
for directory in sorted(os.listdir(data_dir)):
    for img in sorted(os.listdir(os.path.join(data_dir, directory))):
        image_names_train.append(os.path.join(directory, img))
print("No. of images in the train directory of original dataset:", len(image_names_train))

masked_images_train = []
for directory in sorted(os.listdir(masked_dir)):
    for img in sorted(os.listdir(os.path.join(masked_dir, directory))):
        masked_images_train.append(os.path.join(directory, img))
print("No. of images in the train directory of masked dataset:", len(masked_images_train))

print('Test dataset:')
image_names_test = []
for directory in sorted(os.listdir(data_dir_test)):
    for i in sorted(os.listdir(os.path.join(data_dir_test, directory))):
        image_names_test.append(os.path.join(directory, i))
print("No. of images in the test directory of original dataset:", len(image_names_test))

masked_images_test = []
for directory in sorted(os.listdir(masked_dir_test)):
    for i in sorted(os.listdir(os.path.join(masked_dir_test, directory))):
        masked_images_test.append(os.path.join(directory, i))
print("No. of images in the test directory of masked dataset:", len(masked_images_test))

print('Val dataset:')
image_names_val = []
for directory in sorted(os.listdir(data_dir_val)):
    for i in sorted(os.listdir(os.path.join(data_dir_val, directory))):
        image_names_val.append(os.path.join(directory, i))
print("No. of images in the val directory of original dataset:", len(image_names_val))

masked_images_val = []
for directory in sorted(os.listdir(masked_dir_val)):
    for i in sorted(os.listdir(os.path.join(masked_dir_val, directory))):
        masked_images_val.append(os.path.join(directory, i))
print("No. of images in the val directory of masked dataset:", len(masked_images_val))

"""## Model Design

### Generator
"""

OUTPUT_CHANNELS = 3
# Define the downsample function for the generator model
def downsample(filters, size, apply_batchnorm=True):
    # Weight initialization
    initializer = initializer = tf.random_normal_initializer(0., 0.02)

    # Define the model
    model = tf.keras.Sequential()
    model.add(
        tf.keras.layers.Conv2D(
            filters, size, strides=2, padding='same',
            kernel_initializer=initializer, use_bias=False
        )
    )

    # Apply batch normalization if required and add leaky ReLU activation function to the model
    if apply_batchnorm:
        model.add(tf.keras.layers.BatchNormalization(momentum=0.8))

    model.add(tf.keras.layers.LeakyReLU())

    # Return the model
    return model


# Define the upsample function for the generator model
def upsample(filters, size, apply_dropout=False):
    # Weight initialization
    initializer = initializer = tf.random_normal_initializer(0., 0.02)

    # Define the model
    model = tf.keras.Sequential()
    model.add(
        tf.keras.layers.Conv2DTranspose(
            filters, size, strides=2, padding='same',
            kernel_initializer=initializer, use_bias=False
        )
    )

    # Apply batch normalization if required and add ReLU activation function to the model
    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))

    # Apply dropout if required
    if apply_dropout:
        model.add(tf.keras.layers.Dropout(0.5))

    model.add(tf.keras.layers.ReLU())

    # Return the model
    return model


def define_generator(in_shape=(256, 256, 3)):
    # Weight initialization
    inputs = tf.keras.layers.Input(shape=in_shape)

    # Downsampling through the model (encoder) - 8 layers of downsampling (2^8 = 256)
    down_stack = [
        downsample(64, 5, apply_batchnorm=False),  
        downsample(128, 3),  
        downsample(128, 3),  
        downsample(256, 3),  
        downsample(256, 3),  
        downsample(256, 3),  
        downsample(256, 3),  
        downsample(256, 3),  
    ]

    # Upsampling through the model (decoder) - 8 layers of upsampling (2^8 = 256)
    up_stack = [
        upsample(256, 4, apply_dropout=True),  
        upsample(256, 4, apply_dropout=True),  
        upsample(256, 4, apply_dropout=True),  
        upsample(256, 4), 
        upsample(128, 4),  
        upsample(128, 4), 
        upsample(64, 4), 
    ]

    # Weight initialization for the last layer of the model (output layer) - 3 channels for RGB image output
    initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)
    # Define the last layer of the model (output layer)
    # - 3 channels for RGB image output
    # - tanh activation function to ensure output is between -1 and 1
    last = tf.keras.layers.Conv2DTranspose(
        OUTPUT_CHANNELS, 4, strides=2, padding='same',
        kernel_initializer=initializer, activation='tanh'  # (batch_size, 256, 256, 3)
    )

    # Define the model
    x = inputs

    # Downsampling through the model (encoder) - 8 layers of downsampling (2^8 = 256)
    skips = []
    # Loop through the down_stack layers and apply the model to the input
    for down in down_stack:
        x = down(x)
        skips.append(x)

    # Reverse the order of the skips list to allow for the skip connections to be made in the upsampling layers
    skips = reversed(skips[:-1])

    # Upsampling and establishing the skip connections
    for up, skip in zip(up_stack, skips):
        x = up(x)
        # Concatenate the skip connection with the upsampled output from the previous layer
        x = tf.keras.layers.Concatenate()([x, skip])

    # Apply the last layer of the model (output layer) to the output of the final upsampling layer
    x = last(x)

    # Return the model
    model = tf.keras.Model(inputs=inputs, outputs=x, name='Conditional_GAN')
    model.summary()

    return model

"""### Discriminator"""

def define_discriminator(in_shape=(256, 256, 3)):
    init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)
    global_disc_input = tf.keras.layers.Input(shape=in_shape)

    x = tf.keras.layers.Conv2D(32, 5, padding='same', strides=(2, 2), kernel_initializer=init)(global_disc_input)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)

    x = tf.keras.layers.Conv2D(64, 5, padding='same', strides=(2, 2), kernel_initializer=init)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)

    x = tf.keras.layers.Conv2D(128, 5, padding='same', strides=(2, 2), kernel_initializer=init)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)

    x = tf.keras.layers.Conv2D(256, 5, padding='same', strides=(2, 2), kernel_initializer=init)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)

    x = tf.keras.layers.Conv2D(256, 5, padding='same', strides=(2, 2), kernel_initializer=init)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)

    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(512, activation='relu')(x)
    global_disc_output = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=init)(x)

    model = tf.keras.models.Model(inputs=global_disc_input, outputs=global_disc_output, name='Global_Discriminator')
    model.summary()
    return model

def define_PGAN_discriminator(in_shape=(256, 256, 3)):
    # weight initialization
    init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)
    # source image input
    in_src_image = tf.keras.layers.Input(shape=in_shape)
    # C64
    d = tf.keras.layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(in_src_image)
    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)
    # C128
    d = tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)
    d = tf.keras.layers.BatchNormalization()(d)
    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)
    # C256
    d = tf.keras.layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)
    d = tf.keras.layers.BatchNormalization()(d)
    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)
    # C512
    d = tf.keras.layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)
    d = tf.keras.layers.BatchNormalization()(d)
    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)
    # second last output layer
    d = tf.keras.layers.Conv2D(256, (4, 4), padding='same', kernel_initializer=init)(d)
    d = tf.keras.layers.BatchNormalization()(d)
    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)
    # patch output
    d = tf.keras.layers.Conv2D(1, (4, 4), padding='same', kernel_initializer=init)(d)
    patch_out = tf.keras.layers.Activation('sigmoid')(d)
    # define model
    model = tf.keras.models.Model(inputs=in_src_image, outputs=patch_out, name='PGAN_Discriminator')
    model.summary()
    return model


"""### Content Network - Combine the models"""

def define_gan(g_model, dpatch_model, dglobal_model, in_shape=(256, 256, 3)):
    # make weights in the discriminator not trainable
    for layer in dpatch_model.layers:
        if not isinstance(layer, tf.keras.layers.BatchNormalization):
            layer.trainable = False

    for layer in dglobal_model.layers:
        if not isinstance(layer, tf.keras.layers.BatchNormalization):
            layer.trainable = False

    # define the source image
    in_src = tf.keras.layers.Input(shape=in_shape)

    # connect the source image to the generator input
    gen_out = g_model(in_src)

    # connect the source input and generator output to the discriminator input
    dispatch_out = dpatch_model(gen_out)
    disglobal_out = dglobal_model(gen_out)

    # src image as input, generated image and classification output
    model = tf.keras.models.Model(inputs=in_src, outputs=[dispatch_out, disglobal_out, gen_out])

    # compile model
    opt = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
    model.compile(loss=['binary_crossentropy', 'binary_crossentropy', 'mse'], optimizer=opt, loss_weights=[1, 1, 1000])
    model.summary()
    return model


"""### Save models' performance"""

# Saved samples generated by the GAN, original images, damaged images and generated images for comparison
def saveGeneratedSamples(original_images, recon_images, groundtruth_images, epoch):
    n = 5
    plt.figure(figsize=(20, 5))
    for i in range(0, n):
        # Display original
        ax = plt.subplot(3, n, i + 1)
        plt.imshow(np.flip(((original_images + 1) / 2)[i], axis=-1))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        # Title
        ax.title.set_text('Original Images')

        # Display reconstruction
        ax = plt.subplot(3, n, i + 1 + 5)
        plt.imshow(np.flip(((recon_images + 1) / 2)[i], axis=-1))
        # plt.imshow(((recon_images + 1) / 2)[i])
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        # Title
        ax.title.set_text('Generated Images')

        # Display Ground Truth
        ax = plt.subplot(3, n, i + 1 + 10)
        plt.imshow(np.flip(((groundtruth_images + 1) / 2)[i], axis=-1))
        # plt.imshow(((groundtruth_images + 1) / 2)[i])
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        # Title
        ax.title.set_text('Ground Truth Images')
    filename = 'epoch_' + str(epoch) + '_images.png'
    plt.savefig(os.path.join('/mnt/c/Users/chris/2023-mcm-master/src/data/images_v2', filename))
    print('Image saved')
    plt.close()



# Save models so as not to lose progress during training
def saveModels(d_patch, d_global, gan):
    d_patch.save('/mnt/c/Users/chris/2023-mcm-master/src/data/models_v2/d_patch.h5', overwrite=True)
    d_global.save('/mnt/c/Users/chris/2023-mcm-master/src/data/models_v2/d_global.h5', overwrite=True)
    gan.save('/mnt/c/Users/chris/2023-mcm-master/src/data/models_v2/gan.h5', overwrite=True)

def saveGenModel(gen_model, epoch):
    gen_model.save('/mnt/c/Users/chris/2023-mcm-master/src/data/models_v2/g_model_epoch' + str(epoch) + '.h5', overwrite=True)



# Save graphs
def plot_discriminator(dlossreal, dlossfake, dlossreal2, dlossfake2, test=False):
    plt.plot(dlossreal)
    plt.plot(dlossfake)
    plt.plot(dlossreal2)
    plt.plot(dlossfake2)
    plt.title('Discriminator Losses')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    if test:
        plt.legend(['PatchGAN Discriminator test real loss', 'PatchGAN Discriminator test fake loss',
                    'Global Discriminator test real loss', 'Global Discriminator test fake loss'],
                   loc='upper right')
        filename = '/mnt/c/Users/chris/2023-mcm-master/src/data/graphs_v2/discriminator_test_loss_graph_testing.png'
    else:
        plt.legend(['PatchGAN Discriminator train real loss', 'PatchGAN Discriminator train fake loss',
                    'Global Discriminator train real loss', 'Global Discriminator train fake loss'],
                   loc='upper right')
        filename = '/mnt/c/Users/chris/2023-mcm-master/src/data/graphs_v2/discriminator_loss_graph_testing2.png'
    plt.savefig(filename)
    plt.close()


def plot_GAN(glossbce, glossbce2, test=False):
    plt.plot(glossbce)
    plt.plot(glossbce2)
    plt.title('GAN Losses')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    if test:
        plt.legend(['GAN test PatchGAN BCE loss', 'GAN test Global BCE loss'], loc='upper right')
        filename = '/mnt/c/Users/chris/2023-mcm-master/src/data/graphs_v2/GAN_test_loss_graph_testing.png'
    else:
        plt.legend(['GAN train PatchGAN BCE loss', 'GAN train Global BCE loss'], loc='upper right')
        filename = '/mnt/c/Users/chris/2023-mcm-master/src/data/graphs_v2/GAN_loss_graph_testing2.png'

    plt.savefig(filename)
    plt.close()

def plot_psnr(psnr_values, test=False):
    plt.plot(psnr_values)
    plt.title('PSNR')
    plt.xlabel('Epochs')
    plt.ylabel('PSNR (dB)')
    if test:
        plt.legend(['PSNR test'], loc='upper right')
        filename = 'psnr_test_graph.png'
    else:
        plt.legend(['PSNR train'], loc='upper right')
        filename = 'psnr_train_graph.png'
    plt.savefig(filename)
    plt.close()

def plot_mse_loss(glossmse, test=False):
    plt.plot(glossmse)
    plt.title('MSE Loss During Training')
    plt.xlabel('Epochs')
    plt.ylabel('MSE Loss')
    if test:
        plt.legend(['GAN test MSE loss'], loc='upper right')
        filename = '/mnt/c/Users/chris/2023-mcm-master/src/data/graphs_v2/GAN_test_mse_loss_graph_testing.png'
    else:
        plt.legend(['GAN train MSE loss'], loc='upper right')
        filename = '/mnt/c/Users/chris/2023-mcm-master/src/data/graphs_v2/GAN_train_loss_graph_testing2.png'

    plt.savefig(filename)
    plt.close()

def plot_ssim(ssim_values, test=False):
    plt.plot(ssim_values)
    plt.title('SSIM')
    plt.xlabel('Epochs')
    plt.ylabel('SSIM Index')
    if test:
        plt.legend(['SSIM test'], loc='upper right')
        filename = 'ssim_test_graph.png'
    else:
        plt.legend(['SSIM train'], loc='upper right')
        filename = 'ssim_train_graph.png'
    plt.savefig(filename)
    plt.close()

# Generates a random list of images from dataset to use in training batches
# Takes damaged versions of images that will be used to generate fake images
def generate_real_samples(orig_dir, masked_dir, dataset_list, batch_size):
    # Read in all the images of the dataset
    X = dataset_list
    # Create lists of all the images
    original_images = []
    damaged_images = []

    # Randomly sample batch_size
    ix = random.sample(X, batch_size)

    for i in ix:
        # Read in original images
        image_real = cv2.imread(os.path.join(orig_dir, i))
        original_images.append(image_real)
        # Read in damaged versions
        image_damaged = cv2.imread(os.path.join(masked_dir, i))
        damaged_images.append(image_damaged)

    # Convert to numpy arrays of the original images
    original_images = np.asarray(original_images)
    # Normalise the images between -1 and 1
    original_images = (original_images - 127.5) / 127.5

    # Convert to numpy arrays of damaged images
    damaged_images = np.asarray(damaged_images)
    # Normalise the images between -1 and 1
    damaged_images = (damaged_images - 127.5) / 127.5

    return original_images, damaged_images

# Function to calculate PSNR
def calculate_psnr(original_images, restored_images):
    psnr_values = []
    for i in range(len(original_images)):
        psnr = peak_signal_noise_ratio(original_images[i], restored_images[i], data_range=255)
        psnr_values.append(psnr)
    return np.mean(psnr_values)

# Function to calculate SSIM
def calculate_ssim(original_images, restored_images):
    ssim_values = []
    for i in range(len(original_images)):
        ssim = structural_similarity(original_images[i], restored_images[i], data_range=255, multichannel=True)
        ssim_values.append(ssim)
    return np.mean(ssim_values)

"""## Model Training"""

def train(dpatch_model, dglobal_model, g_model, gan_model,
          dataset_list, dataset_test_list, orig_dir, masked_dir, orig_dir_test, masked_dir_test,
          n_epochs=200, n_batch=32, n_batch_test=32, start_epoch=0):

    bat_per_epoch = int(len(dataset_list) / n_batch)
    bat_per_epoch_test = int(len(dataset_test_list) / n_batch_test)

    # Ground Truths
    y_real1 = ones((n_batch, 16, 16, 1))
    y_real2 = ones((n_batch, 1))
    y_fake1 = zeros((n_batch, 16, 16, 1))
    y_fake2 = zeros((n_batch, 1))

    # prepare lists for storing stats each iteration
    dlossreal_epoch = []
    dlossfake_epoch = []
    dlossreal2_epoch = []
    dlossfake2_epoch = []
    glossbce_epoch = []
    glossbce2_epoch = []
    glossmse_epoch = []
    # Test
    dlossreal_epoch_test = []
    dlossfake_epoch_test = []
    dlossreal2_epoch_test = []
    dlossfake2_epoch_test = []
    glossbce_epoch_test = []
    glossbce2_epoch_test = []
    glossmse_epoch_test = []
    # PSNR and SSIM
    psnr_epoch = []
    ssim_epoch = []
    psnr_epoch_test = []
    ssim_epoch_test = []

    epochs = []

    # manually enumerate epochs
    now = datetime.datetime.now().strftime('%d_%m_%y')
    filename = "loss_" + str(now) + ".txt"
    loss_path = '/mnt/c/Users/chris/2023-mcm-master/src/data/logs_v2/' + filename

    with open(loss_path, 'w') as fp:
        for i in range(start_epoch, n_epochs):
            start_time = time.time()
            dlossreal = []
            dlossfake = []
            dlossreal2 = []
            dlossfake2 = []
            glossbce = []
            glossbce2 = []
            glossmse = []
            # Test
            dlossreal_test = []
            dlossfake_test = []
            dlossreal2_test = []
            dlossfake2_test = []
            daccreal_test = []
            daccfake_test = []
            daccreal2_test = []
            daccfake2_test = []
            glossbce_test = []
            glossbce2_test = []
            glossmse_test = []

            psnr = []
            ssim = []
            psnr_test = []
            ssim_test = []

            print('>Epoch: %d of %d' % (i + 1, n_epochs))

            for batch in range(bat_per_epoch):
                # Generate real images and select damaged images
                X_real, damaged_images = generate_real_samples(orig_dir, masked_dir, dataset_list, n_batch)

                # Generate fake images
                X_fake = g_model.predict(damaged_images)

                # update discriminator for real samples
                d_loss_real = dpatch_model.train_on_batch(X_real, y_real1)
                d_loss_real2 = dglobal_model.train_on_batch(X_real, y_real2)
                dlossreal.append(d_loss_real[0])
                dlossreal2.append(d_loss_real2[0])

                # update discriminator for generated samples
                d_loss_fake = dpatch_model.train_on_batch(X_fake, y_fake1)
                d_loss_fake2 = dglobal_model.train_on_batch(X_fake, y_fake2)
                dlossfake.append(d_loss_fake[0])
                dlossfake2.append(d_loss_fake2[0])

                # Update Generator weights
                gloss_all, g_loss_BCE, g_loss_BCE2, g_loss_mse = gan_model.train_on_batch(damaged_images, [y_real1, y_real2, X_real])
                glossbce.append(g_loss_BCE)
                glossbce2.append(g_loss_BCE2)
                glossmse.append(g_loss_mse)

                # Print PSNR and SSIM for training images
                psnr_train = calculate_psnr(X_real, X_fake)
                ssim_train = calculate_ssim(X_real, X_fake)
                psnr.append(psnr_train)
                ssim.append(ssim_train)

            # record history
            dlossreal_epoch.append(np.mean(dlossreal))
            dlossfake_epoch.append(np.mean(dlossfake))
            dlossreal2_epoch.append(np.mean(dlossreal2))
            dlossfake2_epoch.append(np.mean(dlossfake2))

            glossbce_epoch.append(np.mean(glossbce))
            glossbce2_epoch.append(np.mean(glossbce2))
            glossmse_epoch.append(np.mean(glossmse))
            
            psnr_epoch.append(psnr)
            ssim_epoch.append(ssim)

            epochs.append(i)
            finish_time = time.time()
            print('d_real[%.5f] d_fake[%.5f] g_BCE[%.5f] d_real2[%.5f] d_fake2[%.5f] g_BCE2[%.5f] g_mse[%.5f] \n' %
                  (dlossreal_epoch[-1], dlossfake_epoch[-1], glossbce_epoch[-1], dlossreal2_epoch[-1], dlossfake2_epoch[-1],
                   glossbce2_epoch[-1], glossmse_epoch[-1]))
            
            print('> PSNR (Train): %.2f dB - SSIM (Train): %.4f \n' % (psnr_epoch[-1], ssim_epoch[-1]))

            fp.write('Epoch %d :d_real[%.5f] d_fake[%.5f] g_BCE[%.5f] d_real2[%.5f] d_fake2[%.5f] g_BCE2[%.5f] g_mse[%.5f] \n' %
                  (i+1, dlossreal_epoch[-1], dlossfake_epoch[-1], glossbce_epoch[-1], dlossreal2_epoch[-1], dlossfake2_epoch[-1],
                   glossbce2_epoch[-1], glossmse_epoch[-1]))
            
            fp.write('PSNR[%.2f] SSIM[%.4f] \n' % 
                    (i+1, psnr_epoch[-1], ssim_epoch[-1]))
            
            print('Time for epoch: %.0f sec' % (finish_time - start_time))
            saveGenModel(g_model, i+1)
            

            if (i + 1) % 5 == 0:
                plot_discriminator(dlossreal_epoch, dlossfake_epoch, dlossreal2_epoch, dlossfake2_epoch)
                plot_GAN(glossbce_epoch, glossbce2_epoch)
                plot_mse_loss(glossmse_epoch)
                plot_psnr(psnr_epoch)
                plot_ssim(ssim_epoch)
                saveModels(dpatch_model, dglobal_model, gan_model)

                for k in range(bat_per_epoch_test):
                    # Evaluate on test set
                    X_real_test, damaged_images_test = generate_real_samples(orig_dir_test, masked_dir_test, dataset_test_list,
                                                                             n_batch_test)

                    # prepare fake examples
                    X_fake_test = g_model.predict(damaged_images_test)

                    # Update discriminator for real samples
                    d_loss_real_test, d_acc_real_test = dpatch_model.evaluate(X_real_test, y_real1, verbose=0)
                    d_loss_real2_test, d_acc_real2_test = dglobal_model.evaluate(X_real_test, y_real2, verbose=0)
                    dlossreal_test.append(d_loss_real_test)
                    dlossreal2_test.append(d_loss_real2_test)
                    daccreal_test.append(d_acc_real_test)
                    daccreal2_test.append(d_acc_real2_test)

                    # Update discriminator for generated samples
                    d_loss_fake_test, d_acc_fake_test = dpatch_model.evaluate(X_fake_test, y_fake1, verbose=0)
                    d_loss_fake2_test, d_acc_fake2_test = dglobal_model.evaluate(X_fake_test, y_fake2, verbose=0)
                    dlossfake_test.append(d_loss_fake_test)
                    dlossfake2_test.append(d_loss_fake2_test)
                    daccfake_test.append(d_acc_fake_test)
                    daccfake2_test.append(d_acc_fake2_test)

                    # Update Generator weights
                    gloss_all_test, g_loss_BCE_test, g_loss_BCE2_test, g_loss_mse_test = gan_model.train_on_batch(damaged_images_test,
                                                                                              [y_real1, y_real2, X_real_test])
                    glossbce_test.append(g_loss_BCE_test)
                    glossbce2_test.append(g_loss_BCE2_test)
                    glossmse_test.append(g_loss_mse_test)

                    # Calculate PSNR and SSIM
                    psnr = calculate_psnr(X_real_test, X_fake_test)
                    ssim = calculate_ssim(X_real_test, X_fake_test)
                    psnr_test.append(psnr)
                    ssim_test.append(ssim)

                # Record history
                dlossreal_epoch_test.append(np.mean(dlossreal_test))
                dlossfake_epoch_test.append(np.mean(dlossfake_test))
                dlossreal2_epoch_test.append(np.mean(dlossreal2_test))
                dlossfake2_epoch_test.append(np.mean(dlossfake2_test))

                glossbce_epoch_test.append(np.mean(glossbce_test))
                glossbce2_epoch_test.append(np.mean(glossbce2_test))
                glossmse_epoch_test.append(np.mean(glossmse_test))

                psnr_epoch_test.append(psnr_test)
                ssim_epoch_test.append(ssim_test)

                print('> PatchGAN Discrim. Accuracy real: %.0f%%, fake: %.0f%% - Loss real: %.3f, fake: %.3f' % (
                    np.mean(daccreal_test) * 100, np.mean(daccfake_test) * 100, np.mean(dlossreal_test), np.mean(dlossfake_test)))

                print('> Global Discrim. Accuracy real: %.0f%%, fake: %.0f%% - Loss real: %.3f, fake: %.3f' % (
                    np.mean(daccreal2_test) * 100, np.mean(daccfake2_test) * 100, np.mean(dlossreal2_test), np.mean(dlossfake2_test)))

                print('> PSNR (Test): %.2f dB - SSIM (Test): %.4f' % (
                    np.mean(psnr_epoch_test) * 100, np.mean(ssim_epoch_test) * 100))

                fp.write('> PatchGAN Discrim. Accuracy real: %.0f%%, fake: %.0f%% - Loss real: %.3f, fake: %.3f \n' % (
                    np.mean(daccreal_test) * 100, np.mean(daccfake_test) * 100, np.mean(dlossreal_test), np.mean(dlossfake_test)))
                fp.write('> Global Discrim. Accuracy real: %.0f%%, fake: %.0f%% - Loss real: %.3f, fake: %.3f \n' % (
                    np.mean(daccreal2_test) * 100, np.mean(daccfake2_test) * 100, np.mean(dlossreal2_test), np.mean(dlossfake2_test)))
                fp.write('> PSNR (Test): %.2f dB - SSIM (Test): %.4f \n' % (
                    np.mean(psnr_epoch_test) * 100, np.mean(ssim_epoch_test) * 100))


                # Plot the progress
                plot_discriminator(dlossreal_epoch_test, dlossfake_epoch_test, dlossreal2_epoch_test, dlossfake2_epoch_test, True)
                plot_GAN(glossbce_epoch_test, glossbce2_epoch_test, True)
                plot_mse_loss(glossmse_epoch_test, True)
                plot_psnr(psnr_epoch_test, True)
                plot_ssim(ssim_epoch_test, True)
                saveGeneratedSamples(damaged_images_test, X_fake_test, X_real_test, i + 1)

    return dlossreal_epoch, dlossfake_epoch, glossbce_epoch, \
           dlossreal2_epoch, dlossfake2_epoch, glossbce2_epoch, glossmse_epoch, \
           dlossreal_epoch_test, dlossfake_epoch_test, glossbce_epoch_test, \
           dlossreal2_epoch_test, dlossfake2_epoch_test, glossbce2_epoch_test, glossmse_epoch_test

def logLoss(loss_txt):
    now = datetime.datetime.now().strftime('%d_%m_%y')
    filename = "loss_" + str(now) + ".txt"
    loss_path = '/mnt/c/Users/chris/2023-mcm-master/src/data/logs_v2/' + filename
    with open(loss_path, 'w') as fp:
        for row in loss_txt:
            # write each item on a new line
            fp.write("%s\n" % row)

"""## Model Training for train dataset to evaluate GAN model and its completion time of its model training."""

# Create generator network
print('Creating Models...\n')
g_model = define_generator(in_shape=(256, 256, 3))
#g_model = tf.keras.models.load_model('/mnt/c/Users/chris/2023-mcm-master/src/data/models/g_model_epoch89.h5')

# Create discriminator networks
dpatch_model = define_PGAN_discriminator(in_shape=(256, 256, 3))
dglobal_model = define_discriminator(in_shape=(256, 256, 3))
#dpatch_model = tf.keras.models.load_model('/mnt/c/Users/chris/2023-mcm-master/src/data/models/d_patch.h5')
#dglobal_model = tf.keras.models.load_model('/mnt/c/Users/chris/2023-mcm-master/src/data/models/d_global.h5')

opt = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5)
dpatch_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])
dglobal_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])

gan_model = define_gan(g_model, dpatch_model, dglobal_model, in_shape=(256, 256, 3))
#gan_model = tf.keras.models.load_model('/mnt/c/Users/chris/2023-mcm-master/src/data/models/gan.h5')

print('\nModels created!')

# Create directories needed

if not os.path.exists('/mnt/c/Users/chris/2023-mcm-master/src/data/images_v2'):
    print('Creating Images directory...')
    os.mkdir('/mnt/c/Users/chris/2023-mcm-master/src/data/images_v2')

if not os.path.exists('/mnt/c/Users/chris/2023-mcm-master/src/data/graphs_v2'):
    print('Creating Graphs directory...')
    os.mkdir('/mnt/c/Users/chris/2023-mcm-master/src/data/graphs_v2')

if not os.path.exists('/mnt/c/Users/chris/2023-mcm-master/src/data/models_v2'):
    print('Creating Models directory...')
    os.mkdir('/mnt/c/Users/chris/2023-mcm-master/src/data/models_v2')

if not os.path.exists('/mnt/c/Users/chris/2023-mcm-master/src/data/logs_v2'):
    print('Creating Logs directory...')
    os.mkdir('/mnt/c/Users/chris/2023-mcm-master/src/data/logs_v2')

################################################################################
print('Starting Training...')
loss = train(dpatch_model, dglobal_model, g_model, gan_model,
             image_names_train, image_names_test,
             data_dir, masked_dir,
             data_dir_test, masked_dir_test,
             n_epochs=250, n_batch=32, n_batch_test=32
             )

logLoss(loss)

print('Training Done!')
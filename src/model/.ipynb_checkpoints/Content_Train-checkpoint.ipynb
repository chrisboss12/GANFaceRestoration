{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvrlcC1xA7vN",
    "outputId": "715975be-a721-45b9-99a0-5fb0b94b15a0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy.random import rand, randn, randint\n",
    "from numpy import ones, zeros\n",
    "from numpy import vstack\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPool2D, Flatten, Dense, Concatenate, Dropout, BatchNormalization, LeakyReLU, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.initializers.initializers_v1 import RandomNormal\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_Layers = [\n",
    "    # 1\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 5,\n",
    "        'filters': 64,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    # 2\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (2, 2),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    # 3\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (2, 2),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (2, 2),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (4, 4),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (8, 8),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (16, 16),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 256,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    # 4\n",
    "    {\n",
    "        'type': 'de_conv',\n",
    "        'kernal': 4,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (2, 2),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    # 5\n",
    "    {\n",
    "        'type': 'de_conv',\n",
    "        'kernal': 4,\n",
    "        'filters': 64,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (2, 2),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 32,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 3,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'sigmoid'\n",
    "    }\n",
    "]\n",
    "CN_Layers_Reduced = [\n",
    "    # 1\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 5,\n",
    "        'filters': 32,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    # 2\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 64,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (2, 2),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 64,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    # 3\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (2, 2),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (2, 2),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (4, 4),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (8, 8),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (16, 16),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 128,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    # 4\n",
    "    {\n",
    "        'type': 'de_conv',\n",
    "        'kernal': 4,\n",
    "        'filters': 64,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (2, 2),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 64,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    # 5\n",
    "    {\n",
    "        'type': 'de_conv',\n",
    "        'kernal': 4,\n",
    "        'filters': 32,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (2, 2),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 16,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'relu'\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'kernal': 3,\n",
    "        'filters': 3,\n",
    "        'dilation': (1, 1),\n",
    "        'stride': (1, 1),\n",
    "        'activation': 'sigmoid'\n",
    "    }\n",
    "]\n",
    "IMG_DIM = 128\n",
    "\n",
    "\n",
    "def Encoder_Module(input_x, filters, kernal, activation, stride, dilation):\n",
    "    encoding = Conv2D(filters, kernal, stride, padding='same', dilation_rate=dilation)(input_x)\n",
    "    if activation == 'relu':\n",
    "        encoding = BatchNormalization(momentum=0.8)(encoding, training=True)\n",
    "    encoding = Activation(activation)(encoding)\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def Decoder_Module(input_x, filters, kernal, activation, stride, dilation):\n",
    "    decoded = Conv2DTranspose(filters, kernal, stride, padding='same', dilation_rate=dilation)(input_x)\n",
    "    if activation == 'relu':\n",
    "        decoded = BatchNormalization(momentum=0.8)(decoded, training=True)\n",
    "    decoded = Activation(activation)(decoded)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def ContentNetwork():\n",
    "    ContextNetwork_input_layer = Input(shape=(IMG_DIM, IMG_DIM, 3), )\n",
    "\n",
    "    ContextNetwork = ContextNetwork_input_layer\n",
    "\n",
    "    for module in CN_Layers:\n",
    "        if module['type'] == 'conv':\n",
    "            ContextNetwork = Encoder_Module(ContextNetwork, module['filters'], module['kernal'], module['activation'],\n",
    "                                            module['stride'], module['dilation'])\n",
    "        else:\n",
    "            ContextNetwork = Decoder_Module(ContextNetwork, module['filters'], module['kernal'], module['activation'],\n",
    "                                            module['stride'], module['dilation'])\n",
    "\n",
    "    tmp_model = Model(inputs=ContextNetwork_input_layer, outputs=ContextNetwork)\n",
    "\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def ContentNetwork2():\n",
    "    ContextNetwork_input_layer = Input(shape=(IMG_DIM, IMG_DIM, 3), )\n",
    "\n",
    "    ContextNetwork = ContextNetwork_input_layer\n",
    "\n",
    "    for module in CN_Layers_Reduced:\n",
    "        if module['type'] == 'conv':\n",
    "            ContextNetwork = Encoder_Module(ContextNetwork, module['filters'], module['kernal'], module['activation'],\n",
    "                                            module['stride'], module['dilation'])\n",
    "        else:\n",
    "            ContextNetwork = Decoder_Module(ContextNetwork, module['filters'], module['kernal'], module['activation'],\n",
    "                                            module['stride'], module['dilation'])\n",
    "\n",
    "    tmp_model = Model(inputs=ContextNetwork_input_layer, outputs=ContextNetwork)\n",
    "\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def SC_Decoder_Module(input_x, filters, kernal, activation, stride, dilation, sc=None):\n",
    "    decoded = Conv2DTranspose(filters, kernal, stride, padding='same', dilation_rate=dilation)(input_x)\n",
    "    if activation == 'relu':\n",
    "        decoded = BatchNormalization(momentum=0.8)(decoded, training=True)\n",
    "    decoded = Activation(activation)(decoded)\n",
    "\n",
    "    if sc is not None:\n",
    "        decoded = Concatenate()([decoded, sc])\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def skipConnections(input_dim):\n",
    "    ContextNetwork_input_layer = Input(shape=(input_dim, input_dim, 3), )\n",
    "\n",
    "    EM1 = Encoder_Module(input_x=ContextNetwork_input_layer, filters=64, kernal=5, activation='relu', stride=1,\n",
    "                         dilation=1)\n",
    "    EM2 = Encoder_Module(input_x=EM1, filters=128, kernal=3, activation='relu', stride=2,\n",
    "                         dilation=1)\n",
    "    EM3 = Encoder_Module(input_x=EM2, filters=128, kernal=3, activation='relu', stride=1,\n",
    "                         dilation=1)\n",
    "    EM4 = Encoder_Module(input_x=EM3, filters=256, kernal=3, activation='relu', stride=2,\n",
    "                         dilation=1)\n",
    "    EM5 = Encoder_Module(input_x=EM4, filters=256, kernal=3, activation='relu', stride=1,\n",
    "                         dilation=1)\n",
    "    EM6 = Encoder_Module(input_x=EM5, filters=256, kernal=3, activation='relu', stride=1,\n",
    "                         dilation=1)\n",
    "    EM7 = Encoder_Module(input_x=EM6, filters=256, kernal=3, activation='relu', stride=1,\n",
    "                         dilation=2)\n",
    "    EM8 = Encoder_Module(input_x=EM7, filters=256, kernal=3, activation='relu', stride=1,\n",
    "                         dilation=4)\n",
    "    EM9 = Encoder_Module(input_x=EM8, filters=256, kernal=3, activation='relu', stride=1,\n",
    "                         dilation=8)\n",
    "    EM10 = Encoder_Module(input_x=EM9, filters=256, kernal=3, activation='relu', stride=1,\n",
    "                          dilation=16)\n",
    "    EM11 = Encoder_Module(input_x=EM10, filters=256, kernal=3, activation='relu', stride=1,\n",
    "                          dilation=1)\n",
    "    EM12 = Encoder_Module(input_x=EM11, filters=256, kernal=3, activation='relu', stride=1,\n",
    "                          dilation=1)\n",
    "    DM1 = SC_Decoder_Module(input_x=EM12, filters=128, kernal=4, activation='relu', stride=2, dilation=1, sc=EM3)\n",
    "    EM13 = Encoder_Module(input_x=DM1, filters=128, kernal=3, activation='relu', stride=1, dilation=1)\n",
    "    DM2 = SC_Decoder_Module(input_x=EM13, filters=64, kernal=4, activation='relu', stride=2, dilation=1, sc=EM1)\n",
    "    EM14 = Encoder_Module(input_x=DM2, filters=32, kernal=3, activation='relu', stride=1, dilation=1)\n",
    "    EM15 = Encoder_Module(input_x=EM14, filters=3, kernal=3, activation='tanh', stride=1, dilation=1)\n",
    "\n",
    "    model = Model(inputs=ContextNetwork_input_layer, outputs=EM15, name='Generator')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "rZ3HunZEI4lb"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "input = 128\n",
    "\n",
    "GLOBAL_IMG_DIM = 128\n",
    "GLOBAL_MODEL_FILTERS = [32, 64, 128]  # [64, 128, 256]  # , 512, 512]  # , 512]\n",
    "\n",
    "LOCAL_IMG_DIM = 64\n",
    "LOCAL_MODEL_FILTERS = [64, 128, 256, 512]  # , 512]\n",
    "\n",
    "\n",
    "# Define the downsample function for the generator model\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    # Weight initialization\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                               kernel_initializer=initializer, use_bias=False))\n",
    "  \n",
    "    # Apply batch normalization if required and add leaky ReLU activation function to the model\n",
    "    if apply_batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "# Define the upsample function for the generator model\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    # Weight initialization\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "  \n",
    "    # Apply batch normalization if required and add ReLU activation function to the model\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    # Apply dropout if required\n",
    "    if apply_dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "# Generator network\n",
    "def generator_model(in_shape=(256,256,3)):\n",
    "    inputs = tf.keras.layers.Input(in_shape)\n",
    "    \n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "        downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "        downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "        downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "        downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "        downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "        downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "        downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "    \n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "        upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "        upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    \n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "        \n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "        \n",
    "    x = last(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def Discriminator(LocalGlobal):\n",
    "    if LocalGlobal == 'Local':\n",
    "        IMG_DIM = LOCAL_IMG_DIM\n",
    "        FILTERS = LOCAL_MODEL_FILTERS\n",
    "    else:\n",
    "        IMG_DIM = GLOBAL_IMG_DIM\n",
    "        FILTERS = GLOBAL_MODEL_FILTERS\n",
    "\n",
    "    def conv2d(xin, filters, count):\n",
    "        layer = Conv2D(filters, 5, (2, 2), padding='same')(xin)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        if filters != 64:\n",
    "            layer = BatchNormalization(momentum=0.8)(layer)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    input_layer = Input(shape=(IMG_DIM, IMG_DIM, 3))\n",
    "\n",
    "    x = input_layer\n",
    "\n",
    "    for FILTERS, count in zip(FILTERS, range(len(FILTERS))):\n",
    "        x = conv2d(x, FILTERS, count)\n",
    "\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    DiscriminatorModel = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    return DiscriminatorModel\n",
    "\n",
    "\n",
    "def Discriminator2(LocalGlobal):\n",
    "    if LocalGlobal == 'Local':\n",
    "        IMG_DIM = LOCAL_IMG_DIM\n",
    "        FILTERS = LOCAL_MODEL_FILTERS\n",
    "    else:\n",
    "        IMG_DIM = GLOBAL_IMG_DIM\n",
    "        FILTERS = GLOBAL_MODEL_FILTERS\n",
    "\n",
    "    def conv2d(xin, filters):\n",
    "        layer = Conv2D(filters, 5, (2, 2), padding='same')(xin)\n",
    "        if filters != 32:\n",
    "            layer = BatchNormalization(momentum=0.8)(layer)\n",
    "        layer = LeakyReLU(alpha=0.2)(layer)\n",
    "        return layer\n",
    "\n",
    "    input_layer = Input(shape=(IMG_DIM, IMG_DIM, 3))\n",
    "\n",
    "    x = input_layer\n",
    "\n",
    "    for FILTERS in FILTERS:\n",
    "        x = conv2d(x, FILTERS)\n",
    "\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    DiscriminatorModel2 = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    return DiscriminatorModel2\n",
    "\n",
    "\n",
    "# Define the global discriminator model\n",
    "def global_discriminator(in_shape=(256, 256, 3)):\n",
    "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "    global_disc_input = Input(in_shape)\n",
    "    \n",
    "    x = Conv2D(32, 5, padding='same', input_shape=in_shape, strides=(2, 2), kernel_initializer=init)\\\n",
    "        (global_disc_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv2D(64, 5, padding='same', strides=(2, 2), kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, 5, padding='same', strides=(2, 2), kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv2D(256, 5, padding='same', strides=(2, 2), kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv2D(256, 5, padding='same', strides=(2, 2), kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    global_disc_output = Dense(1, activation='sigmoid', kernel_initializer=init)(x)\n",
    "    \n",
    "    model = Model(global_disc_input, outputs=global_disc_output)\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "def multi_patch_discriminator(in_shape=(256, 256, 3)):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "    # source image input\n",
    "    in_src_image = Input(shape=in_shape)\n",
    "    # C64\n",
    "    d = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(in_src_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C128\n",
    "    d = Conv2D(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C256\n",
    "    d = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512\n",
    "    d = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer\n",
    "    d = Conv2D(256, (4, 4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # patch output\n",
    "    d = Conv2D(1, (4, 4), padding='same', kernel_initializer=init)(d)\n",
    "    patch_out = Activation('sigmoid')(d)\n",
    "    # define model\n",
    "    model = Model(in_src_image, patch_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, dpatch_model, dglobal_model, in_shape=(256,256,3)):\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in dpatch_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    for layer in dglobal_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    # define the source image\n",
    "    in_src = Input(shape=in_shape)\n",
    "    \n",
    "    # connect the source image to the generator input\n",
    "    gen_out = g_model(in_src)\n",
    "    \n",
    "    # connect the source input and generator output to the discriminator input\n",
    "    dispatch_out = dpatch_model(gen_out)\n",
    "    disglobal_out = dglobal_model(gen_out)\n",
    "    \n",
    "    # src image as input, generated image and classification output\n",
    "    model = Model(in_src, [dispatch_out,disglobal_out, gen_out])\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy','binary_crossentropy','mse'], optimizer=opt, loss_weights=[1,1,1000])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 256, 256, 64  4864        ['input_11[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 256, 256, 64  256        ['conv2d_65[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_74[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 128, 128, 12  73856       ['activation_31[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 128, 128, 12  512        ['conv2d_66[0][0]']              \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_75[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 128, 128, 12  147584      ['activation_32[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 128, 128, 12  512        ['conv2d_67[0][0]']              \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_76[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 64, 64, 256)  295168      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_40[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_18 (Conv2DTra  (None, 128, 128, 12  524416     ['activation_42[0][0]']          \n",
      " nspose)                        8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 128, 128, 12  512        ['conv2d_transpose_18[0][0]']    \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_86[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 128, 128, 25  0           ['activation_43[0][0]',          \n",
      "                                6)                                'activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 128, 128, 12  295040      ['concatenate_16[0][0]']         \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 128, 128, 12  512        ['conv2d_77[0][0]']              \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_87[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_19 (Conv2DTra  (None, 256, 256, 64  131136     ['activation_44[0][0]']          \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 256, 256, 64  256        ['conv2d_transpose_19[0][0]']    \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_88[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 256, 256, 12  0           ['activation_45[0][0]',          \n",
      "                                8)                                'activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 256, 256, 32  36896       ['concatenate_17[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 256, 256, 32  128        ['conv2d_78[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 256, 256, 32  0           ['batch_normalization_89[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 256, 256, 3)  867         ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 256, 256, 3)  0           ['conv2d_79[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,242,371\n",
      "Trainable params: 6,236,419\n",
      "Non-trainable params: 5,952\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Generator (Functional)         (None, 256, 256, 3)  6242371     ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " model_8 (Functional)           (None, 16, 16, 1)    1745889     ['Generator[0][0]']              \n",
      "                                                                                                  \n",
      " model_9 (Functional)           (None, 1)            11109313    ['Generator[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,097,573\n",
      "Trainable params: 6,239,299\n",
      "Non-trainable params: 12,858,274\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create generator network\n",
    "g_model = skipConnections(256)\n",
    "\n",
    "# Create discriminator networks\n",
    "dpatch_model = multi_patch_discriminator(in_shape=(256,256,3))\n",
    "dglobal_model = global_discriminator(in_shape=(256,256,3))\n",
    "\n",
    "opt = Adam(learning_rate=1e-4, beta_1=0.5)\n",
    "dpatch_model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "dglobal_model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "\n",
    "GAN1 = define_gan(g_model, dpatch_model, dglobal_model, in_shape=(256,256,3))\n",
    "g_model.summary()\n",
    "GAN1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMKI62ECNaW6",
    "outputId": "a60abf23-5816-4357-f0bb-09c13bafb498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GAN Summary:\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model_4 (Functional)           (None, 256, 256, 3)  54425859    ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " model_5 (Functional)           (None, 16, 16, 1)    1745889     ['model_4[0][0]']                \n",
      "                                                                                                  \n",
      " model_6 (Functional)           (None, 1)            11109313    ['model_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 67,281,061\n",
      "Trainable params: 54,417,859\n",
      "Non-trainable params: 12,863,202\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the generator, discriminator, and multi-patch discriminator\n",
    "g_model = generator_model(in_shape=(256,256,3))\n",
    "dpatch_model = multi_patch_discriminator(in_shape=(256,256,3))\n",
    "dglobal_model = global_discriminator(in_shape=(256,256,3))\n",
    "\n",
    "opt = Adam(learning_rate=1e-4, beta_1=0.5)\n",
    "dpatch_model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "dglobal_model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "GAN = define_gan(g_model, dpatch_model, dglobal_model, in_shape=(256,256,3))\n",
    "\n",
    "print(\"\\nGAN Summary:\")\n",
    "GAN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqKQQgkwmtRF"
   },
   "source": [
    "### Generating real and fake samples\n",
    "\n",
    "Real:\n",
    "Chooses a random selection of images from the unmasked images of a specified batch size, returns the images as well as the index of the images so that this can be used to select the corresponding masked images\n",
    "\n",
    "Fake:\n",
    "Using the same indexes as the real images, the corresponding fake images are passed through the GAN to generate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(orig_dir, masked_dir, dataset_list, batch_size):\n",
    "    X = dataset_list\n",
    "    # Declare arrays\n",
    "    original_images=[]\n",
    "    damaged_images=[]\n",
    "    \n",
    "    # Pick random samples\n",
    "    ix= random.sample(X, batch_size)\n",
    "    for i in ix:\n",
    "        # Read in original images\n",
    "        image_real = cv2.imread(os.path.join(orig_dir, i))\n",
    "        original_images.append(image_real)\n",
    "        # Read in damaged versions\n",
    "        image_damaged=cv2.imread(os.path.join(masked_dir, i))\n",
    "        damaged_images.append(image_damaged)\n",
    "    \n",
    "    original_images=np.asarray(original_images)\n",
    "    original_images=(original_images - 127.5) / 127.5\n",
    "    \n",
    "    damaged_images=np.asarray(damaged_images)\n",
    "    damaged_images=(damaged_images - 127.5) / 127.5\n",
    "\n",
    "    return original_images, damaged_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "df7DDV2xmsmD"
   },
   "outputs": [],
   "source": [
    "def saveGeneratedSamples(original_images, recon_images, groundtruth_images, epoch):\n",
    "    if not os.path.exists('Images'):\n",
    "        os.mkdir('Images')\n",
    "\n",
    "    n = 5\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(0, n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(3, n, i + 1)\n",
    "        plt.imshow(((original_images + 1) / 2)[i + 5][:, :, ::-1])\n",
    "#         plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        # Title\n",
    "        ax.title.set_text('Original Images')\n",
    "\n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(3, n, i + 1 + 5)\n",
    "        plt.imshow(((recon_images + 1) / 2)[i + 5][:, :, ::-1])\n",
    "#         plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        # Title\n",
    "        ax.title.set_text('Generated Images')\n",
    "\n",
    "        # Display Ground Truth\n",
    "        ax = plt.subplot(3, n, i + 1 + 10)\n",
    "        plt.imshow(((groundtruth_images + 1) / 2)[i + 5][:, :, ::-1])\n",
    "#         plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        # Title\n",
    "        ax.title.set_text('Ground Truth Images')\n",
    "    ax.set_facecolor(\"white\")\n",
    "    image_format = 'png' # e.g .png, .svg, etc.\n",
    "    filename = f\"Images/epoch_{epoch}_images.png\"\n",
    "    plt.savefig(filename, format=image_format)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(Xin, Yin, DN):\n",
    "\n",
    "    CM_test_x = Xin\n",
    "\n",
    "    y_pred = DN.predict(CM_test_x)\n",
    "    target_names = [\"iceberg\", \"iceberg_masked\"]\n",
    "\n",
    "    cm = confusion_matrix(Yin, y_pred.round())\n",
    "    print('Confusion Matrix')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    print('Classification Report')\n",
    "    print(classification_report(Yin, y_pred.round(), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    " def plot_discriminator(dlossreal, dlossfake, dlossreal2, dlossfake2, steps):\n",
    "    plt.plot(steps, dlossreal)\n",
    "    plt.plot(steps, dlossfake)\n",
    "    plt.plot(steps, dlossreal2)\n",
    "    plt.plot(steps, dlossfake2)\n",
    "    plt.title('Discriminator Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Discriminator train real loss', 'Discriminator train fake loss','Discriminator train real loss2', 'Discriminator train fake loss2'], loc='bottom left')\n",
    "    filename = 'Graphs/discriminator_loss_graph_testing.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "def plot_GAN(glossbce, glossbce2, steps):\n",
    "    plt.plot(steps, glossbce)\n",
    "    plt.plot(steps, glossbce2)\n",
    "    plt.title('GAN Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['GAN train BCE loss', 'GAN train BCE loss2'], loc='bottom left')\n",
    "    filename = 'Graphs/GAN_loss_graph_testing.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7413 images belonging to 22 classes.\n",
      "Found 7686 images belonging to 22 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'C:/Users/chris/2023-mcm-master/src/data/masked_images_split/train'\n",
    "#image_files = data_dir + '/**/*_masked.png'\n",
    "\n",
    "orig_data_dir = 'C:/Users/chris/2023-mcm-master/src/data/dataset_split/train'\n",
    "#orig_image_files = orig_data_dir + '/**/*.png'\n",
    "\n",
    "# Define the ImageDataGenerator for training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load and preprocess training data\n",
    "train_masked = train_datagen.flow_from_directory(\n",
    "    data_dir,  # Pass the directory path, not the file paths\n",
    "    target_size=(256, 256),  # Specify the target size of the images\n",
    "    batch_size=32,\n",
    "    class_mode='input',  # Use 'input' for input modeling\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Load and preprocess training data from orig_data_dir\n",
    "train_orig = train_datagen.flow_from_directory(\n",
    "    orig_data_dir,  # Pass the directory path, not the file paths\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='input',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generator that yields both masked and original images\n",
    "def combined_generator(masked_generator, orig_generator):\n",
    "    while True:\n",
    "        masked_images, _ = masked_generator.next()\n",
    "        orig_images, _ = orig_generator.next()\n",
    "        yield masked_images, orig_images\n",
    "\n",
    "# Create the combined generator\n",
    "combined_train_cgan = combined_generator(train_masked, train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(dpatch_model, dglobal_model, g_model, gan_model,\n",
    "          dataset_list, dataset_test_list,\n",
    "          orig_dir, masked_dir,\n",
    "          n_epochs=200, n_batch=32,n_batch_test=32): \n",
    "    \n",
    "    bat_per_epoch = int(len(dataset_list) / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "\n",
    "    # Ground Truths\n",
    "    y_real1 = ones((n_batch,16,16, 1))\n",
    "    y_real2 = ones((n_batch, 1))\n",
    "    y_fake1 = zeros((n_batch,16,16, 1))\n",
    "    y_fake2 = zeros((n_batch, 1))\n",
    "    \n",
    "    # prepare lists for storing stats each iteration\n",
    "    dlossreal_epoch=[]\n",
    "    dlossfake_epoch=[]\n",
    "    dlossreal2_epoch=[]\n",
    "    dlossfake2_epoch=[]\n",
    "    glossbce_epoch=[]\n",
    "    glossbce2_epoch=[]\n",
    "    glossmae_epoch=[]\n",
    "    steps=[]\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        dlossreal=[]\n",
    "        dlossfake=[]\n",
    "        dlossreal2=[]\n",
    "        dlossfake2=[]\n",
    "        glossbce=[]\n",
    "        glossbce2=[]\n",
    "        glossmae=[]\n",
    "        print('>Epoch: %d' % (i+1))\n",
    "            \n",
    "        for batch in tqdm(range(bat_per_epoch)):\n",
    "            # Generate real images and select damaged images\n",
    "            X_real, damaged_images = generate_real_samples(orig_dir, masked_dir, dataset_list, n_batch)\n",
    "            \n",
    "            resized_images = []\n",
    "            \n",
    "            for image in damaged_images:\n",
    "                # Convert the data type to uint8\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "                resized_image = Image.fromarray(image).resize((256, 256))\n",
    "                resized_images.append(resized_image)\n",
    "                \n",
    "            # Convert the resized images back to numpy array\n",
    "            damaged_images_resized = np.array([np.array(image) for image in resized_images])\n",
    "\n",
    "            # Generate fake images\n",
    "            X_fake = g_model.predict(damaged_images)\n",
    "            \n",
    "            # update discriminator for real samples\n",
    "            d_loss_real = dpatch_model.train_on_batch(X_real, y_real1)\n",
    "            d_loss_real2 = dglobal_model.train_on_batch(X_real,y_real2)            \n",
    "            dlossreal.append(d_loss_real)\n",
    "            dlossreal2.append(d_loss_real2)\n",
    "            \n",
    "            # update discriminator for generated samples\n",
    "            d_loss_fake = dpatch_model.train_on_batch(X_fake, y_fake1)\n",
    "            d_loss_fake2 = dglobal_model.train_on_batch(X_fake,y_fake2)\n",
    "            dlossfake.append(d_loss_fake)\n",
    "            dlossfake2.append(d_loss_fake2)\n",
    "   \n",
    "            # Update Generator weights\n",
    "            gloss_all, g_loss_BCE, g_loss_BCE2, g_loss_mae = gan_model.train_on_batch(damaged_images, [y_real1, y_real2, X_real])\n",
    "            glossbce.append(g_loss_BCE)\n",
    "            glossbce2.append(g_loss_BCE2)\n",
    "            glossmae.append(g_loss_mae)\n",
    "            \n",
    "        # record history\n",
    "        dlossreal_epoch.append(np.mean(dlossreal))\n",
    "        dlossfake_epoch.append(np.mean(dlossfake))\n",
    "        dlossreal2_epoch.append(np.mean(dlossreal2))\n",
    "        dlossfake2_epoch.append(np.mean(dlossfake2))\n",
    "        \n",
    "        glossbce_epoch.append(sum(glossbce)/len(glossbce))\n",
    "        glossbce2_epoch.append(sum(glossbce2)/len(glossbce2))\n",
    "        glossmae_epoch.append(sum(glossmae)/len(glossmae))\n",
    "        steps.append(i)\n",
    "        finish_time = time.time()    \n",
    "        print('d_real[%.5f] d_fake[%.5f] g_BCE[%.5f] d_real2[%.5f] d_fake2[%.5f] g_BCE2[%.5f] g_mse[%.5f]' % (dlossreal_epoch[-1], dlossfake_epoch[-1], glossbce_epoch[-1],dlossreal2_epoch[-1], dlossfake2_epoch[-1], glossbce2_epoch[-1],glossmae_epoch[-1]))        \n",
    "        print('Time for epoch: %.0f sec' % (finish_time-start_time))\n",
    "        if (i+1)%10 == 0:\n",
    "            saveGeneratedSamples(damaged_images, X_fake, X_real, i+1)\n",
    "            plot_discriminator(dlossreal_epoch, dlossfake_epoch, dlossreal2_epoch, dlossfake2_epoch, steps)\n",
    "            plot_GAN(glossbce_epoch, glossbce2_epoch, steps)\n",
    "    return dlossreal_epoch,dlossfake_epoch,glossbce_epoch,glossmae_epoch,dlossreal2_epoch,dlossfake2_epoch,glossbce2_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_dir = 'C:/Users/chris/2023-mcm-master/src/data/masked_images_split/train'\n",
    "orig_dir = 'C:/Users/chris/2023-mcm-master/src/data/dataset_split/train'\n",
    "\n",
    "datasetList = []  # Define the datasetList variable\n",
    "\n",
    "for root, _, files in os.walk(orig_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg') or file.endswith('.png'):  # Add more valid image extensions if needed\n",
    "            file_path = os.path.join(root, file)\n",
    "            datasetList.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/480 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"Generator\" is incompatible with the layer: expected shape=(None, 256, 256, 3), found shape=(None, 1024, 1024, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdpatch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGAN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasetList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43morig_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dpatch_model, dglobal_model, g_model, gan_model, dataset_list, dataset_test_list, orig_dir, masked_dir, n_epochs, n_batch, n_batch_test)\u001b[0m\n\u001b[0;32m     51\u001b[0m damaged_images_resized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39marray(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m resized_images])\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Generate fake images\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m X_fake \u001b[38;5;241m=\u001b[39m \u001b[43mg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdamaged_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# update discriminator for real samples\u001b[39;00m\n\u001b[0;32m     57\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m dpatch_model\u001b[38;5;241m.\u001b[39mtrain_on_batch(X_real, y_real1)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\chris\\anaconda3\\envs\\snowflakes\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"Generator\" is incompatible with the layer: expected shape=(None, 256, 256, 3), found shape=(None, 1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "results=train(dpatch_model, dglobal_model, g_model, GAN, datasetList, [], \n",
    "              orig_dir, masked_dir, n_epochs=10, n_batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "k2tFsRklP6qR",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Calculate the number of steps per epoch\\nsteps_per_epoch = min(len(train_masked), len(train_orig))\\n\\n# Create an instance of the GAN model\\n#gan = gan_model(generator_model(), discriminator_model(), multi_patch_discriminator())\\n\\n# Train the GAN model\\ngan.fit(combined_train_cgan, \\n        epochs=50,\\n        verbose=1)\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Calculate the number of steps per epoch\n",
    "steps_per_epoch = min(len(train_masked), len(train_orig))\n",
    "\n",
    "# Create an instance of the GAN model\n",
    "#gan = gan_model(generator_model(), discriminator_model(), multi_patch_discriminator())\n",
    "\n",
    "# Train the GAN model\n",
    "gan.fit(combined_train_cgan, \n",
    "        epochs=50,\n",
    "        verbose=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
